{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aujourd'hui, il existe beaucoup de produits dit \"d'Intelligence Artificielle (IA)\", ces applications utilisent majoritairement du Machine Learning (ML), qui est un sous-domaine de l'IA, comme vous pouvez le voir ci-dessous. Le machine learning est un champ d'étude qui permet à des programmes de pouvoir résoudre des problèmes à partir de données sans les avoir explicitement programmé. Le deep learning est seulement un des nombreux algorithmes d'apprentissage du machine learning. \n",
    "\n",
    "<img src=\"data/intelligence_artificielle.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous allons nous concentrer sur les problèmatiques d'apprentissage supervisé. Nous allons avoir un jeu de données avec des variables explicatives A et des labels B. Le but de notre algorithme, sera d'apprendre à transformer A en B. Comme vous pouvez le voir ci-dessous, la première étape sera l'entraînement, où l'on donnera des données d'entraînement A et B à un algorithme d'apprentissage. Une fois l'entraînement terminé, l'algorithme nous donne un modèle mathématique. Ce modèle pourra prendre des nouvelles données A' afin de pouvoir estimer leurs résultats B'\n",
    "\n",
    "<img src=\"data/machine_learning.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le deep learning est un algorithme de machine learning basé sur des neurones artificielles. Ces neurones sont des unités de calculs, ils prennt en entrée des informations, les transforment en une valeur et transmettent cette valeur à d'autres neurones.\n",
    "\n",
    "<img src =\"data/neurone.png\" width=\"600\" >\n",
    "\n",
    "Vous pouvez le voir ci-dessous, si l'on met des neurones artificielles en réseau, nous obtenons un réseau de neurones qui permet de prendre en entrée des données A et de changer la représentation de ces données afin de pouvoir transformer plus facilement A en B.\n",
    "\n",
    "<img src =\"data/réseau_de_neurones.png\" width=\"600\" >\n",
    "\n",
    "Initiallement, cet algorithme était censé représenter artificiellement le comportement du cerveau humain. Aujourd'hui, son fonctionnement est bien loin du fonctionnement du cerveau humain même s'il y a encore du vocabulaire hérité de cette inspiration biologique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'avantage du deep learning réside dans la superposition de couches. Cette superposition permet de changer la représentation des données d'entrées de notre réseau pour mettre en valeur différenes caractéristiques de notre jeux de données. Cette nouvelle représentation est élaboré afin de résoudre notre problème plus efficacement. Comme vous pouvez le voir ci-dessous, ça remplace en partie le processus de feature engineering indispensable aux modèles de machines learning classique. \n",
    "\n",
    "<img src =\"data/ml_vs_dl.png\" width=\"700\" >\n",
    "\n",
    "Cette force, a permi le développement d'algorithme de deep learning très robuste capable de mieux comprendre les données non structurés tels que du texte, du son et des images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les architectures de Convolutional Neural Network (CNN)\n",
    "\n",
    "Comme vous l'avons vue précédemment, le principal avantage des réseaux de neurones par rapport aux algorithmes de machine learning traditionnels est que les algorithmes de deep learning apprennent des représentations par eux-mêmes, alors que dans le machine learning traditionnel, on créé des variables à la main sensé être une meilleure représentation des données. Eh bien, c'est vrai pour les réseaux de neurones de type \"fully connceted\" qui ne sont constitués que de couches entièrement connectées. Mais il est très difficile de les entraîner pour des entrées de haute dimension comme les images.\n",
    "\n",
    "Lorsque vous utilisez une architecture de CNN, vous utilisez deux types de fonctionnalités artisanales : les convolutional layers (les filtres de convolution) et les poolign layers (les filtres de regroupement).\n",
    "\n",
    "Le concepteur du CNN pour la classification des images a examiné les données d'entrée (c'est ce que font les ingénieurs en machine learning traditionnels pour inventer des fonctionnalités) et a décidé que des patchs de pixels proches les uns des autres contiennent des informations qui pourraient aider à la classification, et en même temps réduire le nombre de paramètres. Vous pouvez voir l'application d'un patch sur une image ci-dessous.\n",
    "\n",
    "<img src=\"data/application_de_patchs.gif\" width=\"500\" >\n",
    "\n",
    "En effet, l'application de patchs sur une image permet de transformer l'image et de faire ressortir certaines de ces caractéristiques comme vous pouvez le voir ci-dessous.\n",
    "\n",
    "<img src=\"data/filters.png\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but du CNN est de transformer notre image de départ en vecteur qui serait la représentation vectorielle des caractéristiques de notre image. \n",
    "\n",
    "<img src=\"data/conv_nn.png\" width=\"600\" >\n",
    "\n",
    "Une fois que nous avons extrait les caractéristiques de notre image sous forme de vecteur, nous pouvons utiliser cette représentation afin de résoudre un problème lié à notre image. Sur l'illustration ci-dessous, vous pouvez bien voir les deux différentes parties qui consistuent notre réseau. \n",
    "\n",
    "<img src=\"data/CNN.png\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne vous inquiétez pas, vous n'allez pas avoir besoin de connaître toutes les mathématiques qui se cachent derrière les réseaux convolutionel pour pouvoir les utiliser. \n",
    "Grâce à Tensorflow et sa sur-couche Keras, vous pouvez construire un modèle en quelques cliques.\n",
    "\n",
    "### Importation des packages\n",
    "\n",
    "On va commencer par importer les packages nécessaires à l'élaboration de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importations des données\n",
    "\n",
    "Maintenant que nos outils sont chargés, nous allons charger nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) \n",
    "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n",
    "\n",
    "classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "  \n",
    "train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "input_shape = train_set_x_orig[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exemple, nous allons créer un modèle de deep learning capable de reconnaître quel chiffre fait la main une photo. Comme vous pouvez le voir ci-dessous, nous avons 5 classes. \n",
    "\n",
    "<img src=\"data/signes.png\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez directement voir les exemples depuis le jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 0\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(train_set_y_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez voir ci-dessous que nos images sont en trois dimensions 64 x 64 x 3.\n",
    "\n",
    "La première dimension de 64 correspond à la hauteur en pixel de la photo, ici 64 pixels.\n",
    "\n",
    "La deuxième dimension de 64 correspond à la largeur en pixel de la photo, ici 64 pixels.\n",
    "\n",
    "La troisième dimension correspond aux intensités de rouge, de vert et de bleu nécessaire pour former la couleur du pixel.\n",
    "\n",
    "<img src=\"data/RGB-color-model.jpg\" width=\"300\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_x_orig[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer des images en informatique, il n'y a pas une intensité par couleur, ce serait trop complexe à organiser. Pour remédier à cela, nous avons pris trois couleurs (rouge, vert et bleu) et chaque couleur sera une combinaison d'intensité de ces trois couleurs. Le schéma ci-dessous va sûrement vous aider à comprendre le problème. \n",
    "\n",
    "<img src=\"data/image_en_rgb.png\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "Pour des raisons d'optimisation, nous voulons que les données d'entrées de notre modèle soient petit, voir compris entre 0 et 1.\n",
    "\n",
    "La bonne nouvelle, c'est que les intensités d'une image sont toujours comprises entre 0 et 255. Nous allons donc utiliser la min-max normalisation afin de réduire la valeur des intensités de nos images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeur maximum de nos images :  4\n",
      "Valeur minimum de nos images :  244\n",
      "Valeur maximum de nos images normalisées :  0.015686275\n",
      "Valeur minimum de nos images normalisées :  0.95686275\n"
     ]
    }
   ],
   "source": [
    "print('Valeur maximum de nos images : ',np.min(train_set_x_orig))\n",
    "print('Valeur minimum de nos images : ', np.max(train_set_x_orig))\n",
    "\n",
    "X_train = train_set_x_orig.astype('float32')/255\n",
    "X_test = test_set_x_orig.astype('float32')/255\n",
    "\n",
    "print('Valeur maximum de nos images normalisées : ',np.min(X_train))\n",
    "print('Valeur minimum de nos images normalisées : ',np.max(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans nos vecteurs \"train_set_y_orig\" et \"test_set_y_orig\" nous avons le chiffre correspond à l'image de la main.\n",
    "\n",
    "Nous allons transformer nos valeurs en hot vecteur, c'est-à-dire au lieu d'avoir la valeur 5 on aura un vecteur avec un 1 à la 5ème case. \n",
    "\n",
    "<img src=\"data/hot_vecteur.png\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080)\n",
      "(1, 120)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_y_orig.shape)\n",
    "print(test_set_y_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0.]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.eye(6)[train_set_y_orig.reshape(-1)]\n",
    "Y_test = np.eye(6)[test_set_y_orig.reshape(-1)]\n",
    "#Y_train = train_set_y_orig.reshape(1080,)\n",
    "#Y_test = test_set_y_orig.reshape(120,)\n",
    "print(Y_train[150, :])\n",
    "print((train_set_y_orig[:, 150]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction de notre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous allons créer un modèle basique de CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer notre modèle par une couche de convolution. Comme vous pouvez voir ci-dessous, c'est simplement l'application d'un filtre sur notre image pour extraire des caractéristiques.\n",
    "\n",
    "<img src=\"data/convolution_anim.gif\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, \n",
    "                 kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(64,64,3)))\n",
    "model.add(Conv2D(32, \n",
    "                 kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(64,64,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant ajouter une couche de max-pooling. Comme vous pouvez voir ci-dessous, c'est simplement la sélection de l'intensité la plus grande parmi une sélection de pixels.\n",
    "\n",
    "<img src=\"data/pooling_anim.gif\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons extrait des informations de notre image, nous allons transformer ces représentations en vecteur.\n",
    "\n",
    "<img src=\"data/flatten.gif\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir, nous allons utiliser des couches entièrement connectées pour prédire l'appartenance de nos observations à nos classes.\n",
    "\n",
    "<img src=\"data/fully_connected.png\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle est assemblé, nous pouvons maintenant le visualiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 60, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 172806    \n",
      "=================================================================\n",
      "Total params: 182,950\n",
      "Trainable params: 182,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une représentation plus visuelle du modèle.\n",
    "\n",
    "<img src=\"data/baseline.png\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voilà à la dernière étape, l'entraînement de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080 samples, validate on 120 samples\n",
      "Epoch 1/100\n",
      "1080/1080 [==============================] - 0s 344us/sample - loss: 0.2273 - acc: 0.9500 - val_loss: 0.6360 - val_acc: 0.7500\n",
      "Epoch 2/100\n",
      "1080/1080 [==============================] - 0s 217us/sample - loss: 0.2237 - acc: 0.9481 - val_loss: 0.6614 - val_acc: 0.7750\n",
      "Epoch 3/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.2010 - acc: 0.9528 - val_loss: 0.5309 - val_acc: 0.8167\n",
      "Epoch 4/100\n",
      "1080/1080 [==============================] - 0s 245us/sample - loss: 0.1546 - acc: 0.9759 - val_loss: 0.5613 - val_acc: 0.7500\n",
      "Epoch 5/100\n",
      "1080/1080 [==============================] - 0s 244us/sample - loss: 0.1731 - acc: 0.9463 - val_loss: 0.5381 - val_acc: 0.8083\n",
      "Epoch 6/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.1082 - acc: 0.9898 - val_loss: 0.5580 - val_acc: 0.8417\n",
      "Epoch 7/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.1165 - acc: 0.9787 - val_loss: 0.5193 - val_acc: 0.8250\n",
      "Epoch 8/100\n",
      "1080/1080 [==============================] - 0s 234us/sample - loss: 0.0865 - acc: 0.9935 - val_loss: 0.4640 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "1080/1080 [==============================] - 0s 248us/sample - loss: 0.0899 - acc: 0.9880 - val_loss: 0.4665 - val_acc: 0.8250\n",
      "Epoch 10/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0802 - acc: 0.9880 - val_loss: 0.5098 - val_acc: 0.8500\n",
      "Epoch 11/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0653 - acc: 0.9963 - val_loss: 0.4617 - val_acc: 0.8500\n",
      "Epoch 12/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0570 - acc: 0.9981 - val_loss: 0.4749 - val_acc: 0.8417\n",
      "Epoch 13/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0587 - acc: 0.9954 - val_loss: 0.4784 - val_acc: 0.8333\n",
      "Epoch 14/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0502 - acc: 0.9972 - val_loss: 0.4594 - val_acc: 0.8667\n",
      "Epoch 15/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0436 - acc: 0.9981 - val_loss: 0.4631 - val_acc: 0.8500\n",
      "Epoch 16/100\n",
      "1080/1080 [==============================] - 0s 244us/sample - loss: 0.0412 - acc: 0.9981 - val_loss: 0.4561 - val_acc: 0.8583\n",
      "Epoch 17/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0343 - acc: 0.9981 - val_loss: 0.4749 - val_acc: 0.8750\n",
      "Epoch 18/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0324 - acc: 0.9991 - val_loss: 0.4543 - val_acc: 0.8583\n",
      "Epoch 19/100\n",
      "1080/1080 [==============================] - 0s 245us/sample - loss: 0.0315 - acc: 0.9991 - val_loss: 0.4488 - val_acc: 0.8583\n",
      "Epoch 20/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.0289 - acc: 0.9991 - val_loss: 0.4691 - val_acc: 0.8583\n",
      "Epoch 21/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0297 - acc: 0.9991 - val_loss: 0.5026 - val_acc: 0.8250\n",
      "Epoch 22/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0248 - acc: 0.9991 - val_loss: 0.7055 - val_acc: 0.7917\n",
      "Epoch 23/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0250 - acc: 0.9991 - val_loss: 0.4803 - val_acc: 0.8417\n",
      "Epoch 24/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0224 - acc: 0.9991 - val_loss: 0.4607 - val_acc: 0.8583\n",
      "Epoch 25/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0216 - acc: 0.9991 - val_loss: 0.4617 - val_acc: 0.8500\n",
      "Epoch 26/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0199 - acc: 1.0000 - val_loss: 0.4575 - val_acc: 0.8583\n",
      "Epoch 27/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.0188 - acc: 1.0000 - val_loss: 0.4536 - val_acc: 0.8500\n",
      "Epoch 28/100\n",
      "1080/1080 [==============================] - 0s 237us/sample - loss: 0.0179 - acc: 1.0000 - val_loss: 0.4673 - val_acc: 0.8500\n",
      "Epoch 29/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0168 - acc: 1.0000 - val_loss: 0.4615 - val_acc: 0.8667\n",
      "Epoch 30/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0157 - acc: 1.0000 - val_loss: 0.4664 - val_acc: 0.8583\n",
      "Epoch 31/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0153 - acc: 1.0000 - val_loss: 0.5079 - val_acc: 0.8417\n",
      "Epoch 32/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0147 - acc: 1.0000 - val_loss: 0.4772 - val_acc: 0.8667\n",
      "Epoch 33/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0137 - acc: 1.0000 - val_loss: 0.4877 - val_acc: 0.8583\n",
      "Epoch 34/100\n",
      "1080/1080 [==============================] - 0s 238us/sample - loss: 0.0131 - acc: 1.0000 - val_loss: 0.4956 - val_acc: 0.8500\n",
      "Epoch 35/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0128 - acc: 1.0000 - val_loss: 0.4775 - val_acc: 0.8667\n",
      "Epoch 36/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0122 - acc: 1.0000 - val_loss: 0.5180 - val_acc: 0.8333\n",
      "Epoch 37/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0119 - acc: 1.0000 - val_loss: 0.4840 - val_acc: 0.8667\n",
      "Epoch 38/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0113 - acc: 1.0000 - val_loss: 0.4940 - val_acc: 0.8417\n",
      "Epoch 39/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0111 - acc: 1.0000 - val_loss: 0.4821 - val_acc: 0.8583\n",
      "Epoch 40/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0107 - acc: 1.0000 - val_loss: 0.4685 - val_acc: 0.8667\n",
      "Epoch 41/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0104 - acc: 1.0000 - val_loss: 0.5075 - val_acc: 0.8583\n",
      "Epoch 42/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0098 - acc: 1.0000 - val_loss: 0.4931 - val_acc: 0.8667\n",
      "Epoch 43/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0095 - acc: 1.0000 - val_loss: 0.5098 - val_acc: 0.8583\n",
      "Epoch 44/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.0091 - acc: 1.0000 - val_loss: 0.4919 - val_acc: 0.8583\n",
      "Epoch 45/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0092 - acc: 1.0000 - val_loss: 0.5064 - val_acc: 0.8583\n",
      "Epoch 46/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0087 - acc: 1.0000 - val_loss: 0.5152 - val_acc: 0.8500\n",
      "Epoch 47/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 0.4877 - val_acc: 0.8667\n",
      "Epoch 48/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0082 - acc: 1.0000 - val_loss: 0.5020 - val_acc: 0.8583\n",
      "Epoch 49/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0079 - acc: 1.0000 - val_loss: 0.5052 - val_acc: 0.8667\n",
      "Epoch 50/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0079 - acc: 1.0000 - val_loss: 0.5141 - val_acc: 0.8583\n",
      "Epoch 51/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0076 - acc: 1.0000 - val_loss: 0.4991 - val_acc: 0.8500\n",
      "Epoch 52/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0073 - acc: 1.0000 - val_loss: 0.5024 - val_acc: 0.8583\n",
      "Epoch 53/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0072 - acc: 1.0000 - val_loss: 0.5102 - val_acc: 0.8583\n",
      "Epoch 54/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0069 - acc: 1.0000 - val_loss: 0.4880 - val_acc: 0.8667\n",
      "Epoch 55/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.0068 - acc: 1.0000 - val_loss: 0.5097 - val_acc: 0.8583\n",
      "Epoch 56/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0067 - acc: 1.0000 - val_loss: 0.5320 - val_acc: 0.8500\n",
      "Epoch 57/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0065 - acc: 1.0000 - val_loss: 0.5053 - val_acc: 0.8583\n",
      "Epoch 58/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0063 - acc: 1.0000 - val_loss: 0.5330 - val_acc: 0.8667\n",
      "Epoch 59/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.5315 - val_acc: 0.8583\n",
      "Epoch 60/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.5248 - val_acc: 0.8500\n",
      "Epoch 61/100\n",
      "1080/1080 [==============================] - 0s 244us/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.5229 - val_acc: 0.8667\n",
      "Epoch 62/100\n",
      "1080/1080 [==============================] - 0s 246us/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.5279 - val_acc: 0.8583\n",
      "Epoch 63/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0056 - acc: 1.0000 - val_loss: 0.5318 - val_acc: 0.8500\n",
      "Epoch 64/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.5268 - val_acc: 0.8583\n",
      "Epoch 65/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0054 - acc: 1.0000 - val_loss: 0.5142 - val_acc: 0.8667\n",
      "Epoch 66/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 0.5176 - val_acc: 0.8667\n",
      "Epoch 67/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0052 - acc: 1.0000 - val_loss: 0.5125 - val_acc: 0.8667\n",
      "Epoch 68/100\n",
      "1080/1080 [==============================] - 0s 244us/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 0.5351 - val_acc: 0.8583\n",
      "Epoch 69/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0050 - acc: 1.0000 - val_loss: 0.5143 - val_acc: 0.8500\n",
      "Epoch 70/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0050 - acc: 1.0000 - val_loss: 0.5212 - val_acc: 0.8583\n",
      "Epoch 71/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0048 - acc: 1.0000 - val_loss: 0.5218 - val_acc: 0.8667\n",
      "Epoch 72/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0047 - acc: 1.0000 - val_loss: 0.5319 - val_acc: 0.8667\n",
      "Epoch 73/100\n",
      "1080/1080 [==============================] - 0s 245us/sample - loss: 0.0046 - acc: 1.0000 - val_loss: 0.5439 - val_acc: 0.8667\n",
      "Epoch 74/100\n",
      "1080/1080 [==============================] - 0s 244us/sample - loss: 0.0045 - acc: 1.0000 - val_loss: 0.5291 - val_acc: 0.8667\n",
      "Epoch 75/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0045 - acc: 1.0000 - val_loss: 0.5289 - val_acc: 0.8667\n",
      "Epoch 76/100\n",
      "1080/1080 [==============================] - 0s 244us/sample - loss: 0.0044 - acc: 1.0000 - val_loss: 0.5274 - val_acc: 0.8500\n",
      "Epoch 77/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.5309 - val_acc: 0.8583\n",
      "Epoch 78/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.5391 - val_acc: 0.8500\n",
      "Epoch 79/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0041 - acc: 1.0000 - val_loss: 0.5260 - val_acc: 0.8583\n",
      "Epoch 80/100\n",
      "1080/1080 [==============================] - 0s 238us/sample - loss: 0.0041 - acc: 1.0000 - val_loss: 0.5475 - val_acc: 0.8500\n",
      "Epoch 81/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0041 - acc: 1.0000 - val_loss: 0.5363 - val_acc: 0.8500\n",
      "Epoch 82/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 0.5175 - val_acc: 0.8583\n",
      "Epoch 83/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 0.5393 - val_acc: 0.8583\n",
      "Epoch 84/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 0.5374 - val_acc: 0.8667\n",
      "Epoch 85/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5408 - val_acc: 0.8667\n",
      "Epoch 86/100\n",
      "1080/1080 [==============================] - 0s 244us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5511 - val_acc: 0.8583\n",
      "Epoch 87/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5336 - val_acc: 0.8583\n",
      "Epoch 88/100\n",
      "1080/1080 [==============================] - 0s 243us/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 0.5486 - val_acc: 0.8583\n",
      "Epoch 89/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 0.5389 - val_acc: 0.8667\n",
      "Epoch 90/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.5377 - val_acc: 0.8583\n",
      "Epoch 91/100\n",
      "1080/1080 [==============================] - 0s 242us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.5420 - val_acc: 0.8667\n",
      "Epoch 92/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.5452 - val_acc: 0.8583\n",
      "Epoch 93/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.5555 - val_acc: 0.8417\n",
      "Epoch 94/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 0.5383 - val_acc: 0.8500\n",
      "Epoch 95/100\n",
      "1080/1080 [==============================] - 0s 238us/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 0.5661 - val_acc: 0.8583\n",
      "Epoch 96/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.5535 - val_acc: 0.8500\n",
      "Epoch 97/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.5506 - val_acc: 0.8417\n",
      "Epoch 98/100\n",
      "1080/1080 [==============================] - 0s 239us/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5461 - val_acc: 0.8583\n",
      "Epoch 99/100\n",
      "1080/1080 [==============================] - 0s 241us/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5468 - val_acc: 0.8667\n",
      "Epoch 100/100\n",
      "1080/1080 [==============================] - 0s 240us/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5537 - val_acc: 0.8417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d13467668>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=learning_rate), metrics=[\"accuracy\"])\n",
    "model.fit(X_train, Y_train,\n",
    "          epochs = 100,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir ce notebook, nous allons sauvegarder les paramètres de notre modèle si nous voulons l'utiliser à nouveau plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
