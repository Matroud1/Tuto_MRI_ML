{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ny5p4zeN3jn"
   },
   "source": [
    "## La régression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-RLQIF5KIix"
   },
   "source": [
    "Le but de la régression est de créer un modèle qui va savoir estimer une variable continue.\n",
    "\n",
    "Nous allons donner à l'algorithme des données avec les valeurs réelles de la variable à prédire. Pendant l'entraînement, l'algorithme va apprendre à reconnaître la distribution de la variable à prédire ainsi que les caractéristiques qui vont permettre au modèle d'estimer avec le moins d'erreur possible notre cible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYURk3M9MqKQ"
   },
   "source": [
    "## La régression en radiologie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDjEVU_mMvT7"
   },
   "source": [
    "L'âge d'une personne peut être une variable difficile à déterminé lors d'un examen clinique. Le but de cette étude, est d'estimer le plus précisément possible l'âge d'un individu en partant de sont IRM pondérée T1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sélection du matériel\n",
    "\n",
    "Avant de commencer nous allons sélectionné le matériel adapté à nos calculs. Le deep learning à besoin de cartes graphiques (GPU) afin de réduire le temps de calculs.\n",
    "\n",
    "Appuyer sur \"Exécution\" -> \"Modifier le type d'exécution\" \n",
    "\n",
    "Dans \"Accélérateur matériel\" sélectionnez GPU puis appuyer sur enregistrer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cliquez sur le lien ci-dessous : \n",
    "\n",
    "https://drive.google.com/open?id=1bqPk6SasqrUOBB7jfGLb8lRxNdBp9cBY\n",
    "\n",
    "Cliquer droit sur le dossier data et appuyer sur ajouter à mon drive.\n",
    "\n",
    "<img src=\"images/data_google_drive.png\" width=\"800\" >\n",
    "\n",
    "Exécutez la cellule ci-dessous et appuyer sur le lien proposé. \n",
    "\n",
    "Suivez les instructions et copier le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont maintenant dans votre environnement collab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Model architecture\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.layers import ReLU, PReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "import nibabel as nib\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own bibli\n",
    "os.chdir(\"/content/gdrive/My Drive/tp_4/data/utils/\")\n",
    "from callbacks import SaveHyperparameters, SaveMetrics\n",
    "from utils import train_test_val\n",
    "from models import model_Cole\n",
    "from generators import generator_mri_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (193, 229, 193, 1)\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation de nos jeux d'entraînement. Comme vous pouvez le voir ci-dessous, ce jeu de données contient tous les liens où l'on peut trouver nos images ainsi que des informations à propos de nos images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('/content/gdrive/My Drive/tp_4/data/train.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vous pouvez voir ci-dessous la distribution de l'âge du jeu d'entraînement.\n",
    "\n",
    "<img src='data/distrib_age_train.png' >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation de nos jeux de test. Comme vous pouvez le voir ci-dessous, ce jeu de données contient tous les liens où l'on peut trouver nos images ainsi que des informations à propos de nos images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('/content/gdrive/My Drive/tp_4/data/test.csv')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez voir ci-dessous la distribution de l'âge du jeu de test.\n",
    "\n",
    "<img src='data/distrib_age_test.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les intensités de nos images IRM ont été normalisé par la technique vue dans le notebook 2.\n",
    "\n",
    "On est passé de cette représentation de nos intensité pour les données d'entraînement :\n",
    "\n",
    "<img src='data/t1_unnorm_distribution.png' > \n",
    "          \n",
    "A cette distribution d'intensité après normalisation : \n",
    "\n",
    "\n",
    "<img src='data/t1_norm_distribution.png' > \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Souvent en deep learning les jeux d'entraînement et de test sont trop gros pour être charger en mémoire. \n",
    "\n",
    "Pour entraîner nos modèles nous sommes obligé d'entraîner nos modèles par mini-batch. Un batch est une fraction de notre jeu d'entraînement. \n",
    "\n",
    "<img src='data/mini-batch.png' >\n",
    "\n",
    "Prenons un jeu d'entraînement X avec $n_x$ le nombre de variables et $m$ le nombre d'exemples. \n",
    "\n",
    "Imaginons que notre jeu de données est égale à $m$=50 000 000, l'entraînement de ce modèle est impossible. Pour rendre l'entraînement possible, nous allons entraîner plusieurs mini-batch $X^{\\{i\\}}$ afin d'entraîner tous nos exemples séquentiellement.\n",
    "\n",
    "Pour simuler ces mini-batch nous allons utiliser des générateurs qui vont fournir à l'algorithme les données par séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons créer de nouveau jeu de données contenant seulement les variables utilent à l'entraînement de notre modèle\n",
    "train = data_train.loc[:, ('t1_norm', 'age')]\n",
    "\n",
    "# Nous allons créer de nouveau jeu de données contenant seulement les variables utilent aux tests de notre modèle\n",
    "test = data_test.loc[:, ('t1_norm', 'age')]\n",
    "\n",
    "# On précise le nombre d'exemple que va contenir chaque batch\n",
    "batch_size = 4\n",
    "\n",
    "generator_train = generator_mri_regression(list_path=train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "generator_test = generator_mri_regression(list_path=test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La profondeur des architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture de base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un projet de machine learning nous allons toujours commencé par entraîner un modèle très simple pour voir à quel point problème est compris. \n",
    "\n",
    "Plus tard chaque amélioration pourra être comparé avec ce modèle dit \"de base\" afin de voir l'amélioration de ces performances.\n",
    "\n",
    "Comme vous pouvez le voir sur le schéma ci-dessous le modèle est composé de deux couches de convolution, une couche de maxpooling et une couche. Ensuite nous allons transformer nos matrices en vecteur c'est la partie \"flatten\" sur le schéma et pour finir nous aurons une couche complétement connectés.\n",
    "\n",
    "<img src=\"data/baseline1.png\" >\n",
    "\n",
    "Ci-dessous vous pouvez voir l'implémentation en keras du schéma ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def base_model(size):\n",
    "    \"\"\"\n",
    "    Model of \"Predictiong brain age with deep learning from raw imaging data results in a reliable and heritable biomarker\"\n",
    "\n",
    "    Inputs:\n",
    "        - model_options:\n",
    "        - weights_path: path to where weights should be saved\n",
    "    Output:\n",
    "        - nets = list of NeuralNets (CNN1, CNN2)def Unet_3D_model(modalities, patch_size, filters=32, dropout_rate=0.2):\n",
    "\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=size,\n",
    "                     name=\"conv_1_1\"))\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     name=\"conv_1_2\"))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           strides=(2, 2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=1,\n",
    "                    name=\"d_2\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la fonction \"summary\" nous avons le détail des paramètres qui compose notre modèle couche par couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model(SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce partie nous initialisons nos \"callbacks\", ces fonctiosn servent à sauvegarder des informations tout au long de l'entraînement de nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'baseline_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "training_path = '/content/'+test_name\n",
    "os.mkdir(training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/content/\"+test_name+\"/model_saved/\"\n",
    "os.mkdir(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_mean_absolute_error:.2f}.hdf5\", monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons compilé notre modèle en spécifiant la fonction de coût que nous voulons utiliser pour entraîner notre modèle, la fonction d'optimisation que nous voulons utiliser pour mettre à jour nos paramètres et la métrique de performance qui sera calculer pour suivre l'évolution des performances tout au long de notre entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model.compile(loss=['mae'],\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, notre modèle est prêt pour l'entraînement. \n",
    "\n",
    "Nous donnons le générateur d'entraînement, le générateur de test, et le nombre d'itérations d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=5, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle s'entraîne, cet entraînement peut être très long. Pendant l'entraînement nous allons sauvegarder automatiquement les modèles les plus performant et l'évolution des performances durant l'entraînement. \n",
    "\n",
    "Vous pouvez ci-dessous charger un modèle entraîner pendant 100 itérations avec le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/content/gdrive/My Drive/tp_4/data/models/model-baseline_it-100_data-100.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données sauvegarder on peut visualiser l'évolution des performances au cours de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_base = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/training_data/metrics_baseline_100_100.csv\")\n",
    "plt.plot(data_training_base.loc[1:, 'epoch'], data_training_base.loc[1:, 'train_loss'], label='Training Mean Absolute Error')\n",
    "plt.plot(data_training_base.loc[0:, 'epoch'], data_training_base.loc[0:, 'val_loss'], label='Validation Mean Absolute Error', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir d'après le graphique ci-dessus que notre modèle souffre de sous-entraînement. Notre modèle n'est pas assez complexe pour comprendre le phénomène à modéliser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle intermédiaire\n",
    "\n",
    "Nous allons donc utiliser un modèle plus complexe afin de voir s'il arrive à mieux comprendre les phénomènes à utiliser. Vous pouvez voir le schéma de notre nouveau modèle ci-dessous.\n",
    "\n",
    "<img src=\"data/V2_model.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def model_baseV2(size):\n",
    "    \"\"\"\n",
    "    Model of \"Predictiong brain age with deep learning from raw imaging data results in a reliable and heritable biomarker\"\n",
    "\n",
    "    Inputs:\n",
    "        - model_options:\n",
    "        - weights_path: path to where weights should be saved\n",
    "    Output:\n",
    "        - nets = list of NeuralNets (CNN1, CNN2)def Unet_3D_model(modalities, patch_size, filters=32, dropout_rate=0.2):\n",
    "\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=size,\n",
    "                     name=\"conv_1_1\"))\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     name=\"conv_1_2\"))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           strides=(2, 2, 2)))\n",
    "    model.add(Conv3D(filters=16,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=size,\n",
    "                     name=\"conv_2_1\"))\n",
    "    model.add(Conv3D(filters=16,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     name=\"conv_2_2\"))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           strides=(2, 2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=1,\n",
    "                    name=\"d_2\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2 = model_baseV2(SIZE)\n",
    "model_V2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'model_V2_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "training_path = '/content/'+test_name\n",
    "os.mkdir(training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/content/\"+test_name+\"/model_saved/\"\n",
    "os.mkdir(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_mean_absolute_error:.2f}.hdf5\", monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_V2.compile(loss=['mae'],\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=5, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle s'entraîne, cet entraînement peut être très long. Pendant l'entraînement nous allons sauvegarder automatiquement les modèles les plus performant et l'évolution des performances durant l'entraînement. \n",
    "\n",
    "Vous pouvez ci-dessous charger un modèle entraîner pendant 100 itérations avec le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2.load_weights('/content/gdrive/My Drive/tp_4/data/models/model-v2_it-100_data-100.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données sauvegarder on peut visualiser l'évolution des performances au cours de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_V2 = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/training_data/metrics_v2_100_100.csv\")\n",
    "plt.plot(data_training_V2.loc[:, 'epoch'], data_training_V2.loc[:, 'train_loss'], label='Training Mean Absolute Error')\n",
    "plt.plot(data_training_V2.loc[:, 'epoch'], data_training_V2.loc[:, 'val_loss'], label='Validation Mean Absolute Error', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir d'après le graphique ci-dessus que notre modèle comprend mieux notre phénomène que le modèle précédent. Nous allons voir ci prendre un modèle plus complexe nous donnera plus de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture plus élaboré\n",
    "\n",
    "Ce modèle est plus performant que le précédent en ayant 5 blocks au lieu de 2 précédemment (voir le schéma ci-dessous).\n",
    "\n",
    "<img src=\"data/cole_model1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block of Cole model\n",
    "def block_model_Cole(model, nb_block):\n",
    "    model.add(Conv3D(filters=8*pow(2,nb_block),\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     name=\"conv_\"+str(nb_block+1)+\"_1\"))\n",
    "    model.add(Conv3D(filters=8*pow(2,nb_block),\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     padding='same',\n",
    "                     activation=None,\n",
    "                     name=\"conv_\"+str(nb_block+1)+\"_2\"))\n",
    "    model.add(BatchNormalization(name=\"bn_\"+str(nb_block+1)+\"_3\", axis=1))\n",
    "    model.add(ReLU(name=\"relu_conv_\"+str(nb_block+1)+\"_4\"))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           strides=(2, 2, 2)))\n",
    "    return model\n",
    "\n",
    "# Model\n",
    "def cole_model(size):\n",
    "    \"\"\"\n",
    "    Model of \"Predictiong brain age with deep learning from raw imaging data results in a reliable and heritable biomarker\"\n",
    "\n",
    "    Inputs:\n",
    "        - model_options:\n",
    "        - weights_path: path to where weights should be saved\n",
    "    Output:\n",
    "        - nets = list of NeuralNets (CNN1, CNN2)def Unet_3D_model(modalities, patch_size, filters=32, dropout_rate=0.2):\n",
    "\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=size,\n",
    "                     name=\"conv_1_1\"))\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     padding='same',\n",
    "                     activation=None,\n",
    "                     name=\"conv_1_2\"))\n",
    "    model.add(BatchNormalization(name='bn_1_3', axis=1))\n",
    "    model.add(ReLU(name='relu_conv_1_4'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           strides=(2, 2, 2)))\n",
    "    model = block_model_Cole(model, 1)\n",
    "    model = block_model_Cole(model, 2)\n",
    "    model = block_model_Cole(model, 3)\n",
    "    model = block_model_Cole(model, 4)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=1,\n",
    "                    name=\"d_6\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole =  cole_model(SIZE)\n",
    "model_cole.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_cole.compile(loss=['mae'],\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'cole_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "training_path = '/content/'+test_name\n",
    "os.mkdir(training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/content/\"+test_name+\"/model_saved/\"\n",
    "os.mkdir(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_mean_absolute_error:.2f}.hdf5\", monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=5, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle s'entraîne, cet entraînement peut être très long. Pendant l'entraînement nous allons sauvegarder automatiquement les modèles les plus performant et l'évolution des performances durant l'entraînement. \n",
    "\n",
    "Vous pouvez ci-dessous charger un modèle entraîner pendant 100 itérations avec le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole.load_weights('/content/gdrive/My Drive/tp_4/data/models/model-cole_it-100_data-100.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données sauvegarder on peut visualiser l'évolution des performances au cours de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_cole = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/training_data/metrics_cole_100_100.csv\")\n",
    "plt.plot(data_training_cole.loc[:, 'epoch'], data_training_cole.loc[:, 'train_loss'], label='Training Mean Absolute Error')\n",
    "plt.plot(data_training_cole.loc[:, 'epoch'], data_training_cole.loc[:, 'val_loss'], label='Validation Mean Absolute Error', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que le modèle donne des performances du même ordre de grandeur que le modèle précédent. Complexifier le modèle davantage ne le rendra pas plus performant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etude des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va tracer l'évolution des performances de nos modèles en fonction du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_training_base.loc[:, 'epoch'], data_training_base.loc[:, 'val_loss'], label='Validation Mean Absolute Error base model', color='blue')\n",
    "plt.plot(data_training_V2.loc[:, 'epoch'], data_training_V2.loc[:, 'val_loss'], label='Validation Mean Absolute Error V2 model', color='green')\n",
    "plt.plot(data_training_cole.loc[:, 'epoch'], data_training_cole.loc[:, 'val_loss'], label='Validation Mean Absolute Error Cole model', color='red')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), shadow=True, ncol=1)\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut déduire que la première architecture n'est pas assez complexe pour résoudre notre problème. La deuxième architecture comprend mieux le problème. Un modèle plus complexe ne comprend pas mieux le problème.\n",
    "\n",
    "Il faudrait surement ajouter des données afin que notre algorithme puisse s'entraîner davantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le nombre de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement d'un jeu de données de 500 sujets sain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/IXI.csv\")\n",
    "\n",
    "# Nous allons créer de nouveau jeu de données contenant seulement les variables utilent à l'entraînement de notre modèle\n",
    "data = data_frame.loc[:, ('t1_norm', 'age')]\n",
    "\n",
    "train_matrix, test_matrix, val_matrix = train_test_val(data)\n",
    "\n",
    "# On précise le nombre d'exemple que va contenir chaque batch\n",
    "batch_size = 4\n",
    "\n",
    "generator_train = generator_mri_regression(list_path=train_matrix, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "generator_test = generator_mri_regression(list_path=test_matrix, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture intermédiaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2 = model_baseV2(SIZE)\n",
    "model_V2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'model_V2_ixitrain_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "training_path = '/content/'+test_name\n",
    "os.mkdir(training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/content/\"+test_name+\"/model_saved/\"\n",
    "os.mkdir(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_mean_absolute_error:.2f}.hdf5\", monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_V2.compile(loss=['mae'],\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=5, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle s'entraîne, cet entraînement peut être très long. Pendant l'entraînement nous allons sauvegarder automatiquement les modèles les plus performant et l'évolution des performances durant l'entraînement. \n",
    "\n",
    "Vous pouvez ci-dessous charger un modèle entraîner pendant 100 itérations avec le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2.load_weights('/content/gdrive/My Drive/tp_4/data/models/model-v2_it-100_data-500.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données sauvegarder on peut visualiser l'évolution des performances au cours de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_V2 = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/training_data/metrics_v2_100_500.csv\")\n",
    "plt.plot(data_training_V2.loc[1:, 'epoch'], data_training_V2.loc[1:, 'train_loss'], label='Training Mean Absolute Error')\n",
    "plt.plot(data_training_V2.loc[0:, 'epoch'], data_training_V2.loc[0:, 'val_loss'], label='Validation Mean Absolute Error', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ajout de données a permi de résoudre notre problème plus précisément. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture plus élaborée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole =  cole_model(SIZE)\n",
    "model_cole.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_cole.compile(loss=['mae'],\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'cole_ixi_train_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "training_path = '/content/'+test_name\n",
    "os.mkdir(training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/content/\"+test_name+\"/model_saved/\"\n",
    "os.mkdir(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_mean_absolute_error:.2f}.hdf5\", monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=5, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle s'entraîne, cet entraînement peut être très long. Pendant l'entraînement nous allons sauvegarder automatiquement les modèles les plus performant et l'évolution des performances durant l'entraînement. \n",
    "\n",
    "Vous pouvez ci-dessous charger un modèle entraîner pendant 100 itérations avec le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole.load_weights('/content/gdrive/My Drive/tp_4/data/models/model-cole_it-100_data-500.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données sauvegarder on peut visualiser l'évolution des performances au cours de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_cole = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/training_data/metrics_cole_100_500.csv\")\n",
    "plt.plot(data_training_cole.loc[1:, 'epoch'], data_training_cole.loc[1:, 'train_loss'], label='Training Mean Absolute Error')\n",
    "plt.plot(data_training_cole.loc[0:, 'epoch'], data_training_cole.loc[0:, 'val_loss'], label='Validation Mean Absolute Error', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ajout de données a permi de résoudre notre problème plus précisément. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etude des résultats\n",
    "\n",
    "On va tracer l'évolution des performances de nos modèles en fonction du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_training_V2.loc[:, 'epoch'], data_training_V2.loc[:, 'val_loss'], label='Validation Mean Absolute Error V2 model', color='green')\n",
    "plt.plot(data_training_cole.loc[:, 'epoch'], data_training_cole.loc[:, 'val_loss'], label='Validation Mean Absolute Error Cole model', color='red')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), shadow=True, ncol=1)\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que nos modèles ont des performances similaire. Mais un modèle plus complexe met en générale plus de temps pour s'entraîner qu'un modèle plus simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre d'itérations\n",
    "\n",
    "Nous allons étudier si augmenter le nombre d'itérations va augmenter la performance du modèle le plus complexe par rapport au modèle plus simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle intermédiaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2 = model_baseV2(SIZE)\n",
    "model_V2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'it300_model_V2_ixitrain_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "training_path = '/content/'+test_name\n",
    "os.mkdir(training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/content/\"+test_name+\"/model_saved/\"\n",
    "os.mkdir(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_mean_absolute_error:.2f}.hdf5\", monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_V2.compile(loss=['mae'],\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=5, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle s'entraîne, cet entraînement peut être très long. Pendant l'entraînement nous allons sauvegarder automatiquement les modèles les plus performant et l'évolution des performances durant l'entraînement. \n",
    "\n",
    "Vous pouvez ci-dessous charger un modèle entraîner pendant 500 itérations avec le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_V2.load_weights('/content/gdrive/My Drive/tp_4/data/models/model-v2_it-300_data-500.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données sauvegarder on peut visualiser l'évolution des performances au cours de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_V2 = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/training_data/metrics_v2_it-300_data-500.csv\")\n",
    "plt.plot(data_training_V2.loc[1:, 'epoch'], data_training_V2.loc[1:, 'train_loss'], label='Training Mean Absolute Error')\n",
    "plt.plot(data_training_V2.loc[0:, 'epoch'], data_training_V2.loc[0:, 'val_loss'], label='Validation Mean Absolute Error', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que l'augmentation de l'itération augmente la performance sur le jeu d'entraînement sans toutefois augmenter la performance sur le jeu de test. C'est le signe du sur-entraînement de notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle plus élaboré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole =  cole_model(SIZE)\n",
    "model_cole.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_cole.compile(loss=['mae'],\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'it300_cole_ixi_train_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "training_path = '/content/'+test_name\n",
    "os.mkdir(training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/content/\"+test_name+\"/model_saved/\"\n",
    "os.mkdir(filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_mean_absolute_error:.2f}.hdf5\", monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=5, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle s'entraîne, cet entraînement peut être très long. Pendant l'entraînement nous allons sauvegarder automatiquement les modèles les plus performant et l'évolution des performances durant l'entraînement. \n",
    "\n",
    "Vous pouvez ci-dessous charger un modèle entraîner pendant 500 itérations avec le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cole.load_weights('/content/gdrive/My Drive/tp_4/data/models/model-cole_it-300_-data.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données sauvegarder on peut visualiser l'évolution des performances au cours de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_cole = pd.read_csv(\"/content/gdrive/My Drive/tp_4/data/training_data/metrics_cole_300_500.csv\")\n",
    "plt.plot(data_training_cole.loc[1:, 'epoch'], data_training_cole.loc[1:, 'train_loss'], label='Training Mean Absolute Error')\n",
    "plt.plot(data_training_cole.loc[0:, 'epoch'], data_training_cole.loc[0:, 'val_loss'], label='Validation Mean Absolute Error', color='green')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que l'augmentation de l'itération augmente la performance sur le jeu d'entraînement sans toutefois augmenter la performance sur le jeu de test. C'est le signe du sur-entraînement de notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etude des résultats\n",
    "\n",
    "On va tracer l'évolution des performances de nos modèles en fonction du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_training_V2.loc[:, 'epoch'], data_training_V2.loc[:, 'val_loss'], label='Validation Mean Absolute Error V2 model', color='green')\n",
    "plt.plot(data_training_cole.loc[:, 'epoch'], data_training_cole.loc[:, 'val_loss'], label='Validation Mean Absolute Error Cole model', color='red')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), shadow=True, ncol=1)\n",
    "plt.title('Evolution of the error during the training using raw data')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (age)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que plus d'itération ont permi à notre modèle plus complexe d'être plus performant que notre premier modèle. Nos modèles souffre de sur-apprentissage. Pour augmenter nos performances par la suite il faudrait utiliser des techniques de régularisation, ou d'obtenir plus de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "La segmentation automatique.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
