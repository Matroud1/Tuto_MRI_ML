{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ny5p4zeN3jn"
   },
   "source": [
    "## La segmentation en Radiologie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-RLQIF5KIix"
   },
   "source": [
    "La segmentation est un procédé de traitement d'image qui a pour but de regrouper des pixels en régions afin de reprérer des structures dans l'image. Par exemple, ça peut être utiliser pour séparer le fond d'une personne prise en photo. L'humain va donc décider pixel par pixel ceux qui appartiennent au fond et ceux qui appartiennent à la personne. \n",
    "\n",
    "Cette tâche est longue, pas très agréable et peu connaître une grande [variabilité inter-opérateur](https://www.sciencedirect.com/science/article/pii/S0360301604001294). Le but serait d'utiliser un algorithme de machine learning capable de regrouper automatiquement et parfaitement les groupes de pixels selon les structures que l'on souhaite segmenter. \n",
    "\n",
    "Les radiologues sont souvent soumis à cette exercice afin de contrôler le volume de certaines structures, détecter des lesions ou des tumeurs. Ils passent donc beaucoup de temps sur ces tâches qui ont moins de valeur ajoutée qu'un diagnostique clinique ou que la prise en charge d'un patient. La segmentation automatique de structure cérébrale a donc un grand intérêt pour facilité la routine clinique des radiologues. \n",
    "\n",
    "Aujourd'hui, il existe des outils semi-automatique où l'algorithme défini de façon grossière les zones des structures et le radiologue n'a cas bien rétablir les limites des structures à ségmenter. Avec l'évolution du deep learning ainsi qu'avec l'accumulation de données d'imagerie médicale ces problèmes pourront être résolu par des algorithmes performant, rapide et qui ne souffirait pas d'une variabilité inter-opérateur. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYURk3M9MqKQ"
   },
   "source": [
    "## Les architectures de segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDjEVU_mMvT7"
   },
   "source": [
    "Les architectures qui marche aujourd'hui très bien sont les architectures en U. Vous avez peut être déjà entendu parler de [l'U-net](https://arxiv.org/abs/1505.04597). C'est une architecture qui a été créer pour faire de la segmentation de photo en deux dimensions. Ici, nous travaillons sur des données volumique. Nous allons utiliser le [V-net](https://arxiv.org/abs/1606.04797) qui est une déclinaison de l'U-net pour effectuer des segmentations en trois dimensions. \n",
    "\n",
    "Vous pouvez voir ci-dessous une représentation de l'architecture du V-net utilisé pour la ségmentation automatique d'image en trois dimensions.\n",
    "\n",
    "<center>\n",
    "<img src='https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99F807505C2C5D1B15'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlQTpSrhXUUA"
   },
   "source": [
    "## Segmentation de l'Hippocampe\n",
    "\n",
    "L'hippocampe est une structure cérébrale, il appartient notamment au système limbique et joue un rôle central dans la mémoire et la navigation spatiale. Le but de ce notebook sera de réussir à segmenter automatiquement cette structure du cerveau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Model architecture\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, ELU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Conv3DTranspose\n",
    "from tensorflow.keras.layers import ReLU, PReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "import nibabel as nib\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own bibli\n",
    "sys.path.append(\"/NAS/deathrow/morgan/dev/bibli/\")\n",
    "from callbacks import SaveHyperparameters, SaveMetrics\n",
    "from utils import train_test_val\n",
    "from models import model_Cole\n",
    "from generators import generator_mri_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (193, 229, 193, 1)\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sub</th>\n",
       "      <th>t1</th>\n",
       "      <th>t1_norm</th>\n",
       "      <th>hippo_mask</th>\n",
       "      <th>gm_mask</th>\n",
       "      <th>wm_mask</th>\n",
       "      <th>brain_mask</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>sub-053</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>413</td>\n",
       "      <td>sub-413</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>586</td>\n",
       "      <td>sub-IXI586</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>sub-IXI256</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261</td>\n",
       "      <td>sub-IXI261</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id         sub                                                 t1  \\\n",
       "0   53     sub-053  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  413     sub-413  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  586  sub-IXI586  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  256  sub-IXI256  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  261  sub-IXI261  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                             t1_norm  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                          hippo_mask  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                             gm_mask  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                             wm_mask  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                          brain_mask  sex   age  \n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    0  53.0  \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    1  59.0  \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    0  34.0  \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    0  27.0  \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    0  34.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('/NAS/deathrow/morgan/TP_IRM/Tuto_MRI_ML/tp_5/data/train.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>t1</th>\n",
       "      <th>t1_norm</th>\n",
       "      <th>hippo_mask</th>\n",
       "      <th>gm_mask</th>\n",
       "      <th>wm_mask</th>\n",
       "      <th>brain_mask</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-IXI331</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-064</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-IXI426</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-IXI230</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-IXI201</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>/NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sub                                                 t1  \\\n",
       "0  sub-IXI331  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1     sub-064  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  sub-IXI426  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  sub-IXI230  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  sub-IXI201  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                             t1_norm  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                          hippo_mask  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                             gm_mask  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                             wm_mask  \\\n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...   \n",
       "\n",
       "                                          brain_mask  sex   age  \n",
       "0  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    0  23.0  \n",
       "1  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    0  20.0  \n",
       "2  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    1  23.0  \n",
       "3  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    1  21.0  \n",
       "4  /NAS/deathrow/morgan/bids_proc/IXI/freesurfer/...    1  22.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('/NAS/deathrow/morgan/TP_IRM/Tuto_MRI_ML/tp_4/data/test.csv')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les intensités de nos images IRM ont été normalisé par la technique vue dans le notebook 2.\n",
    "\n",
    "On est passé de cette représentation de nos intensité pour les données d'entraînement :\n",
    "\n",
    "<img src='data/t1_unnorm_distribution.png' > \n",
    "          \n",
    "A cette distribution d'intensité après normalisation : \n",
    "\n",
    "\n",
    "<img src='data/t1_norm_distribution.png' > \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Souvent en deep learning les jeux d'entraînement et de test sont trop gros pour être charger en mémoire. \n",
    "\n",
    "Pour entraîner nos modèles nous sommes obligé d'entraîner nos modèles par mini-batch. Un batch est une fraction de notre jeu d'entraînement. \n",
    "\n",
    "<img src='data/mini-batch.png' >\n",
    "\n",
    "Prenons un jeu d'entraînement X avec $n_x$ le nombre de variables et $m$ le nombre d'exemples. \n",
    "\n",
    "Imaginons que notre jeu de données est égale à $m$=50 000 000, l'entraînement de ce modèle est impossible. Pour rendre l'entraînement possible, nous allons entraîner plusieurs mini-batch $X^{\\{i\\}}$ afin d'entraîner tous nos exemples séquentiellement.\n",
    "\n",
    "Pour simuler ces mini-batch nous allons utiliser des générateurs qui vont fournir à l'algorithme les données par séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons créer de nouveau jeu de données contenant seulement les variables utilent à l'entraînement de notre modèle\n",
    "train = data_train.loc[:, ('t1_norm', 'hippo_mask')]\n",
    "\n",
    "# Nous allons créer de nouveau jeu de données contenant seulement les variables utilent aux tests de notre modèle\n",
    "test = data_test.loc[:, ('t1_norm', 'hippo_mask')]\n",
    "\n",
    "# On précise le nombre d'exemple que va contenir chaque batch\n",
    "batch_size = 4\n",
    "\n",
    "generator_train = generator_mri_segmentation(list_path=train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "generator_test = generator_mri_segmentation(list_path=test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CEN_model(image_size):\n",
    "    \"\"\"\n",
    "    CNN for semgentation of multplie sclerosis lesion.\n",
    "    This architecture was inspired by paper of Brosch et al. \n",
    "    \"Deep Convolutional Encoder Networks for Multiple Sclerosis Lesion Segmentation\"\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(filters=32,\n",
    "                kernel_size=(9, 9, 9),\n",
    "                name=\"conv1\",\n",
    "                activation=None,    \n",
    "                padding=\"valid\",\n",
    "                input_shape=image_size))\n",
    "    model.add(ELU(alpha=1.0))\n",
    "    model.add(Conv3DTranspose(filters=1,\n",
    "                kernel_size=(9, 9, 9),\n",
    "                name=\"Tconv2\",\n",
    "                activation=\"sigmoid\"))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0220 14:23:39.962900 139968603064064 deprecation.py:506] From /home/morgan/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 185, 221, 185, 32) 23360     \n",
      "_________________________________________________________________\n",
      "elu (ELU)                    (None, 185, 221, 185, 32) 0         \n",
      "_________________________________________________________________\n",
      "Tconv2 (Conv3DTranspose)     (None, 193, 229, 193, 1)  23329     \n",
      "=================================================================\n",
      "Total params: 46,689\n",
      "Trainable params: 46,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CEN_model(SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back initialization\n",
    "test_name = 'baseline_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "model_name = 'tp_5'\n",
    "\n",
    "training_path = '/NAS/deathrow/morgan/training_data/'+model_name+'/'+test_name\n",
    "os.system('mkdir '+training_path)\n",
    "\n",
    "hyperparameters = SaveHyperparameters(training_path, LEARNING_RATE, batch_size, 100, False, 0, 0, 'test_baseline')\n",
    "\n",
    "save_metrics = SaveMetrics(training_path)\n",
    "\n",
    "filepath=\"/NAS/deathrow/morgan/training_data/\"+model_name+\"/\"+test_name+\"/model_saved/\"\n",
    "os.system(\"mkdir \"+filepath)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath+\"HCP-test_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, save_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model.compile(loss=binary_crossentropy,\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.4766 - acc: 0.8478 \n",
      "Epoch 00001: val_acc improved from inf to 0.84725, saving model to /NAS/deathrow/morgan/training_data/tp_5/baseline_2020-02-20_14:33:48/model_saved/HCP-test_weights-improvement-01-0.85.hdf5\n",
      "25/25 [==============================] - 268s 11s/step - loss: 0.4802 - acc: 0.8476 - val_loss: 0.4832 - val_acc: 0.8472\n",
      "Epoch 2/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.4205 - acc: 0.8509 \n",
      "Epoch 00002: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 263s 11s/step - loss: 0.4167 - acc: 0.8514 - val_loss: 0.4082 - val_acc: 0.8510\n",
      "Epoch 3/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.3551 - acc: 0.8551 \n",
      "Epoch 00003: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 253s 10s/step - loss: 0.3504 - acc: 0.8553 - val_loss: 0.2820 - val_acc: 0.8854\n",
      "Epoch 4/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.2022 - acc: 0.9646 \n",
      "Epoch 00004: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.1988 - acc: 0.9658 - val_loss: 0.1526 - val_acc: 0.9953\n",
      "Epoch 5/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.1122 - acc: 0.9969 \n",
      "Epoch 00005: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 254s 10s/step - loss: 0.1140 - acc: 0.9969 - val_loss: 0.0972 - val_acc: 0.9977\n",
      "Epoch 6/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0761 - acc: 0.9981 \n",
      "Epoch 00006: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 255s 10s/step - loss: 0.0769 - acc: 0.9981 - val_loss: 0.0738 - val_acc: 0.9983\n",
      "Epoch 7/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0603 - acc: 0.9984 \n",
      "Epoch 00007: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0599 - acc: 0.9984 - val_loss: 0.0544 - val_acc: 0.9984\n",
      "Epoch 8/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0503 - acc: 0.9986 \n",
      "Epoch 00008: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 255s 10s/step - loss: 0.0496 - acc: 0.9986 - val_loss: 0.0454 - val_acc: 0.9985\n",
      "Epoch 9/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0402 - acc: 0.9986 \n",
      "Epoch 00009: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0410 - acc: 0.9986 - val_loss: 0.0395 - val_acc: 0.9986\n",
      "Epoch 10/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0340 - acc: 0.9986 \n",
      "Epoch 00010: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 255s 10s/step - loss: 0.0347 - acc: 0.9986 - val_loss: 0.0349 - val_acc: 0.9986\n",
      "Epoch 11/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0312 - acc: 0.9987 \n",
      "Epoch 00011: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0311 - acc: 0.9987 - val_loss: 0.0356 - val_acc: 0.9987\n",
      "Epoch 12/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0329 - acc: 0.9986 \n",
      "Epoch 00012: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 259s 10s/step - loss: 0.0324 - acc: 0.9986 - val_loss: 0.0313 - val_acc: 0.9986\n",
      "Epoch 13/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0280 - acc: 0.9987 \n",
      "Epoch 00013: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0277 - acc: 0.9987 - val_loss: 0.0285 - val_acc: 0.9987\n",
      "Epoch 14/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0275 - acc: 0.9987 \n",
      "Epoch 00014: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 255s 10s/step - loss: 0.0273 - acc: 0.9987 - val_loss: 0.0292 - val_acc: 0.9987\n",
      "Epoch 15/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0243 - acc: 0.9987 \n",
      "Epoch 00015: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 258s 10s/step - loss: 0.0241 - acc: 0.9987 - val_loss: 0.0244 - val_acc: 0.9987\n",
      "Epoch 16/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0230 - acc: 0.9987 \n",
      "Epoch 00016: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 254s 10s/step - loss: 0.0231 - acc: 0.9987 - val_loss: 0.0263 - val_acc: 0.9987\n",
      "Epoch 17/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0249 - acc: 0.9987 \n",
      "Epoch 00017: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 268s 11s/step - loss: 0.0243 - acc: 0.9987 - val_loss: 0.0228 - val_acc: 0.9987\n",
      "Epoch 18/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0203 - acc: 0.9987 \n",
      "Epoch 00018: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 258s 10s/step - loss: 0.0200 - acc: 0.9987 - val_loss: 0.0223 - val_acc: 0.9987\n",
      "Epoch 19/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0232 - acc: 0.9987 \n",
      "Epoch 00019: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 260s 10s/step - loss: 0.0229 - acc: 0.9987 - val_loss: 0.0215 - val_acc: 0.9987\n",
      "Epoch 20/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0193 - acc: 0.9987 \n",
      "Epoch 00020: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0191 - acc: 0.9987 - val_loss: 0.0206 - val_acc: 0.9986\n",
      "Epoch 21/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0212 - acc: 0.9987 \n",
      "Epoch 00021: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 257s 10s/step - loss: 0.0209 - acc: 0.9987 - val_loss: 0.0203 - val_acc: 0.9987\n",
      "Epoch 22/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0190 - acc: 0.9987 \n",
      "Epoch 00022: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 259s 10s/step - loss: 0.0188 - acc: 0.9987 - val_loss: 0.0187 - val_acc: 0.9987\n",
      "Epoch 23/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0200 - acc: 0.9987 \n",
      "Epoch 00023: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0198 - acc: 0.9987 - val_loss: 0.0194 - val_acc: 0.9987\n",
      "Epoch 24/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0177 - acc: 0.9987 \n",
      "Epoch 00024: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 260s 10s/step - loss: 0.0176 - acc: 0.9987 - val_loss: 0.0200 - val_acc: 0.9987\n",
      "Epoch 25/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0197 - acc: 0.9987 \n",
      "Epoch 00025: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 253s 10s/step - loss: 0.0199 - acc: 0.9987 - val_loss: 0.0186 - val_acc: 0.9987\n",
      "Epoch 26/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0164 - acc: 0.9987 \n",
      "Epoch 00026: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0168 - acc: 0.9987 - val_loss: 0.0175 - val_acc: 0.9986\n",
      "Epoch 27/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0171 - acc: 0.9987 \n",
      "Epoch 00027: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 254s 10s/step - loss: 0.0169 - acc: 0.9987 - val_loss: 0.0175 - val_acc: 0.9987\n",
      "Epoch 28/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0179 - acc: 0.9987 \n",
      "Epoch 00028: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 255s 10s/step - loss: 0.0178 - acc: 0.9987 - val_loss: 0.0173 - val_acc: 0.9987\n",
      "Epoch 29/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0176 - acc: 0.9987 \n",
      "Epoch 00029: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 257s 10s/step - loss: 0.0174 - acc: 0.9987 - val_loss: 0.0171 - val_acc: 0.9987\n",
      "Epoch 30/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0165 - acc: 0.9987 \n",
      "Epoch 00030: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 252s 10s/step - loss: 0.0176 - acc: 0.9987 - val_loss: 0.0167 - val_acc: 0.9987\n",
      "Epoch 31/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0172 - acc: 0.9987 \n",
      "Epoch 00031: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0172 - acc: 0.9987 - val_loss: 0.0162 - val_acc: 0.9987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0155 - acc: 0.9986 \n",
      "Epoch 00032: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 252s 10s/step - loss: 0.0155 - acc: 0.9986 - val_loss: 0.0169 - val_acc: 0.9986\n",
      "Epoch 33/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0160 - acc: 0.9987 \n",
      "Epoch 00033: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 257s 10s/step - loss: 0.0159 - acc: 0.9987 - val_loss: 0.0159 - val_acc: 0.9987\n",
      "Epoch 34/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0147 - acc: 0.9987 \n",
      "Epoch 00034: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 257s 10s/step - loss: 0.0147 - acc: 0.9987 - val_loss: 0.0157 - val_acc: 0.9986\n",
      "Epoch 35/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0162 - acc: 0.9987 \n",
      "Epoch 00035: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 255s 10s/step - loss: 0.0161 - acc: 0.9987 - val_loss: 0.0148 - val_acc: 0.9986\n",
      "Epoch 36/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0154 - acc: 0.9987 \n",
      "Epoch 00036: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0151 - acc: 0.9987 - val_loss: 0.0153 - val_acc: 0.9986\n",
      "Epoch 37/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0157 - acc: 0.9986 \n",
      "Epoch 00037: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0158 - acc: 0.9987 - val_loss: 0.0159 - val_acc: 0.9987\n",
      "Epoch 38/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0151 - acc: 0.9987 \n",
      "Epoch 00038: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 252s 10s/step - loss: 0.0149 - acc: 0.9987 - val_loss: 0.0149 - val_acc: 0.9987\n",
      "Epoch 39/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0172 - acc: 0.9987 \n",
      "Epoch 00039: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0172 - acc: 0.9987 - val_loss: 0.0156 - val_acc: 0.9986\n",
      "Epoch 40/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0134 - acc: 0.9987 \n",
      "Epoch 00040: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 253s 10s/step - loss: 0.0134 - acc: 0.9987 - val_loss: 0.0139 - val_acc: 0.9986\n",
      "Epoch 41/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0150 - acc: 0.9987 \n",
      "Epoch 00041: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 258s 10s/step - loss: 0.0150 - acc: 0.9987 - val_loss: 0.0146 - val_acc: 0.9986\n",
      "Epoch 42/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0149 - acc: 0.9986 \n",
      "Epoch 00042: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 260s 10s/step - loss: 0.0148 - acc: 0.9986 - val_loss: 0.0146 - val_acc: 0.9986\n",
      "Epoch 43/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0150 - acc: 0.9987 \n",
      "Epoch 00043: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 257s 10s/step - loss: 0.0148 - acc: 0.9987 - val_loss: 0.0146 - val_acc: 0.9986\n",
      "Epoch 44/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0136 - acc: 0.9987 \n",
      "Epoch 00044: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 260s 10s/step - loss: 0.0137 - acc: 0.9987 - val_loss: 0.0146 - val_acc: 0.9987\n",
      "Epoch 45/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0141 - acc: 0.9987 \n",
      "Epoch 00045: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 259s 10s/step - loss: 0.0140 - acc: 0.9987 - val_loss: 0.0137 - val_acc: 0.9986\n",
      "Epoch 46/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0156 - acc: 0.9987 \n",
      "Epoch 00046: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 255s 10s/step - loss: 0.0154 - acc: 0.9987 - val_loss: 0.0142 - val_acc: 0.9987\n",
      "Epoch 47/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0140 - acc: 0.9987 \n",
      "Epoch 00047: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 254s 10s/step - loss: 0.0139 - acc: 0.9987 - val_loss: 0.0142 - val_acc: 0.9987\n",
      "Epoch 48/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0139 - acc: 0.9986 \n",
      "Epoch 00048: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 259s 10s/step - loss: 0.0137 - acc: 0.9986 - val_loss: 0.0149 - val_acc: 0.9987\n",
      "Epoch 49/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0141 - acc: 0.9987 \n",
      "Epoch 00049: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0141 - acc: 0.9987 - val_loss: 0.0136 - val_acc: 0.9986\n",
      "Epoch 50/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0156 - acc: 0.9986 \n",
      "Epoch 00050: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 260s 10s/step - loss: 0.0154 - acc: 0.9986 - val_loss: 0.0141 - val_acc: 0.9986\n",
      "Epoch 51/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0125 - acc: 0.9987 \n",
      "Epoch 00051: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 258s 10s/step - loss: 0.0125 - acc: 0.9987 - val_loss: 0.0138 - val_acc: 0.9986\n",
      "Epoch 52/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0148 - acc: 0.9986 \n",
      "Epoch 00052: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 256s 10s/step - loss: 0.0146 - acc: 0.9986 - val_loss: 0.0140 - val_acc: 0.9986\n",
      "Epoch 53/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0135 - acc: 0.9986 \n",
      "Epoch 00053: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0135 - acc: 0.9986 - val_loss: 0.0132 - val_acc: 0.9986\n",
      "Epoch 54/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0124 - acc: 0.9987 \n",
      "Epoch 00054: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 252s 10s/step - loss: 0.0123 - acc: 0.9986 - val_loss: 0.0134 - val_acc: 0.9987\n",
      "Epoch 55/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0146 - acc: 0.9987 \n",
      "Epoch 00055: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 252s 10s/step - loss: 0.0145 - acc: 0.9987 - val_loss: 0.0131 - val_acc: 0.9986\n",
      "Epoch 56/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0137 - acc: 0.9986 \n",
      "Epoch 00056: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0137 - acc: 0.9987 - val_loss: 0.0135 - val_acc: 0.9986\n",
      "Epoch 57/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0137 - acc: 0.9987 \n",
      "Epoch 00057: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0136 - acc: 0.9987 - val_loss: 0.0132 - val_acc: 0.9986\n",
      "Epoch 58/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0127 - acc: 0.9986 \n",
      "Epoch 00058: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0127 - acc: 0.9986 - val_loss: 0.0133 - val_acc: 0.9986\n",
      "Epoch 59/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0134 - acc: 0.9986 \n",
      "Epoch 00059: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0133 - acc: 0.9986 - val_loss: 0.0130 - val_acc: 0.9986\n",
      "Epoch 60/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0131 - acc: 0.9987 \n",
      "Epoch 00060: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0130 - acc: 0.9987 - val_loss: 0.0134 - val_acc: 0.9986\n",
      "Epoch 61/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0135 - acc: 0.9986 \n",
      "Epoch 00061: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0134 - acc: 0.9986 - val_loss: 0.0133 - val_acc: 0.9986\n",
      "Epoch 62/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0133 - acc: 0.9987 \n",
      "Epoch 00062: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0134 - acc: 0.9987 - val_loss: 0.0131 - val_acc: 0.9986\n",
      "Epoch 63/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0131 - acc: 0.9987 \n",
      "Epoch 00063: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0129 - acc: 0.9987 - val_loss: 0.0131 - val_acc: 0.9986\n",
      "Epoch 64/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0132 - acc: 0.9986 \n",
      "Epoch 00064: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0131 - acc: 0.9986 - val_loss: 0.0128 - val_acc: 0.9986\n",
      "Epoch 65/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0131 - acc: 0.9986 \n",
      "Epoch 00065: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0131 - acc: 0.9986 - val_loss: 0.0135 - val_acc: 0.9986\n",
      "Epoch 66/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0123 - acc: 0.9987 \n",
      "Epoch 00066: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0122 - acc: 0.9987 - val_loss: 0.0129 - val_acc: 0.9986\n",
      "Epoch 67/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0148 - acc: 0.9986 \n",
      "Epoch 00067: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0146 - acc: 0.9986 - val_loss: 0.0126 - val_acc: 0.9985\n",
      "Epoch 68/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0115 - acc: 0.9986 \n",
      "Epoch 00068: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0124 - acc: 0.9986 - val_loss: 0.0128 - val_acc: 0.9986\n",
      "Epoch 69/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0119 - acc: 0.9987 \n",
      "Epoch 00069: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0120 - acc: 0.9986 - val_loss: 0.0127 - val_acc: 0.9986\n",
      "Epoch 70/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0133 - acc: 0.9986 \n",
      "Epoch 00070: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0134 - acc: 0.9986 - val_loss: 0.0124 - val_acc: 0.9986\n",
      "Epoch 71/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0121 - acc: 0.9987 \n",
      "Epoch 00071: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0121 - acc: 0.9987 - val_loss: 0.0129 - val_acc: 0.9986\n",
      "Epoch 72/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0130 - acc: 0.9986 \n",
      "Epoch 00072: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0129 - acc: 0.9986 - val_loss: 0.0125 - val_acc: 0.9987\n",
      "Epoch 73/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0129 - acc: 0.9986 \n",
      "Epoch 00073: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0128 - acc: 0.9986 - val_loss: 0.0121 - val_acc: 0.9986\n",
      "Epoch 74/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0128 - acc: 0.9986 \n",
      "Epoch 00074: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0127 - acc: 0.9986 - val_loss: 0.0125 - val_acc: 0.9986\n",
      "Epoch 75/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0118 - acc: 0.9986 \n",
      "Epoch 00075: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0121 - acc: 0.9986 - val_loss: 0.0130 - val_acc: 0.9986\n",
      "Epoch 76/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0134 - acc: 0.9986 \n",
      "Epoch 00076: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0132 - acc: 0.9986 - val_loss: 0.0128 - val_acc: 0.9986\n",
      "Epoch 77/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0120 - acc: 0.9986 \n",
      "Epoch 00077: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0120 - acc: 0.9986 - val_loss: 0.0128 - val_acc: 0.9986\n",
      "Epoch 78/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0120 - acc: 0.9986 \n",
      "Epoch 00078: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 252s 10s/step - loss: 0.0119 - acc: 0.9986 - val_loss: 0.0127 - val_acc: 0.9986\n",
      "Epoch 79/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0124 - acc: 0.9986 \n",
      "Epoch 00079: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0126 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9986\n",
      "Epoch 80/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0126 - acc: 0.9986 \n",
      "Epoch 00080: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0126 - acc: 0.9986 - val_loss: 0.0126 - val_acc: 0.9986\n",
      "Epoch 81/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0123 - acc: 0.9986 \n",
      "Epoch 00081: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0123 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9986\n",
      "Epoch 82/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0133 - acc: 0.9986 \n",
      "Epoch 00082: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0132 - acc: 0.9986 - val_loss: 0.0118 - val_acc: 0.9986\n",
      "Epoch 83/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0120 - acc: 0.9986 \n",
      "Epoch 00083: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0122 - acc: 0.9986 - val_loss: 0.0125 - val_acc: 0.9986\n",
      "Epoch 84/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0113 - acc: 0.9986 \n",
      "Epoch 00084: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0120 - acc: 0.9986 - val_loss: 0.0122 - val_acc: 0.9986\n",
      "Epoch 85/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0115 - acc: 0.9986 \n",
      "Epoch 00085: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0114 - acc: 0.9986 - val_loss: 0.0124 - val_acc: 0.9986\n",
      "Epoch 86/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0130 - acc: 0.9986 \n",
      "Epoch 00086: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0131 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9986\n",
      "Epoch 87/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0117 - acc: 0.9986 \n",
      "Epoch 00087: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0117 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9986\n",
      "Epoch 88/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0129 - acc: 0.9986 \n",
      "Epoch 00088: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0128 - acc: 0.9986 - val_loss: 0.0118 - val_acc: 0.9986\n",
      "Epoch 89/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0113 - acc: 0.9986 \n",
      "Epoch 00089: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0112 - acc: 0.9986 - val_loss: 0.0121 - val_acc: 0.9986\n",
      "Epoch 90/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0130 - acc: 0.9986 \n",
      "Epoch 00090: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0129 - acc: 0.9986 - val_loss: 0.0120 - val_acc: 0.9986\n",
      "Epoch 91/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0118 - acc: 0.9986 \n",
      "Epoch 00091: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0118 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9986\n",
      "Epoch 92/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0125 - acc: 0.9986 \n",
      "Epoch 00092: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0125 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9986\n",
      "Epoch 93/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0112 - acc: 0.9986 \n",
      "Epoch 00093: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0112 - acc: 0.9986 - val_loss: 0.0122 - val_acc: 0.9986\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0117 - acc: 0.9986 \n",
      "Epoch 00094: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0117 - acc: 0.9986 - val_loss: 0.0118 - val_acc: 0.9986\n",
      "Epoch 95/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0129 - acc: 0.9987 \n",
      "Epoch 00095: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0128 - acc: 0.9986 - val_loss: 0.0121 - val_acc: 0.9985\n",
      "Epoch 96/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0120 - acc: 0.9986 \n",
      "Epoch 00096: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0119 - acc: 0.9986 - val_loss: 0.0117 - val_acc: 0.9986\n",
      "Epoch 97/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0111 - acc: 0.9986 \n",
      "Epoch 00097: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0111 - acc: 0.9986 - val_loss: 0.0117 - val_acc: 0.9986\n",
      "Epoch 98/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0129 - acc: 0.9986 \n",
      "Epoch 00098: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0129 - acc: 0.9986 - val_loss: 0.0121 - val_acc: 0.9985\n",
      "Epoch 99/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0113 - acc: 0.9986 \n",
      "Epoch 00099: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0113 - acc: 0.9986 - val_loss: 0.0117 - val_acc: 0.9986\n",
      "Epoch 100/100\n",
      "24/25 [===========================>..] - ETA: 5s - loss: 0.0115 - acc: 0.9986 \n",
      "Epoch 00100: val_acc did not improve from 0.84725\n",
      "25/25 [==============================] - 251s 10s/step - loss: 0.0115 - acc: 0.9986 - val_loss: 0.0120 - val_acc: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4c785f46a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=generator_train.loader(),\n",
    "                    steps_per_epoch=generator_train.get_len(),\n",
    "                    epochs=100, \n",
    "                    verbose=1,\n",
    "                    validation_data=generator_test.loader(),\n",
    "                    validation_steps=generator_test.get_len(),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_predict = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture V-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def xavier_initializer_convolution(shape, dist='uniform', lambda_initializer=True):\n",
    "    \"\"\"\n",
    "    Xavier initializer for N-D convolution patches. input_activations = patch_volume * in_channels;\n",
    "    output_activations = patch_volume * out_channels; Uniform: lim = sqrt(3/(input_activations + output_activations))\n",
    "    Normal: stddev =  sqrt(6/(input_activations + output_activations))\n",
    "    :param shape: The shape of the convolution patch i.e. spatial_shape + [input_channels, output_channels]. The order of\n",
    "    input_channels and output_channels is irrelevant, hence this can be used to initialize deconvolution parameters.\n",
    "    :param dist: A string either 'uniform' or 'normal' determining the type of distribution\n",
    "    :param lambda_initializer: Whether to return the initial actual values of the parameters (True) or placeholders that\n",
    "    are initialized when the session is initiated\n",
    "    :return: A numpy araray with the initial values for the parameters in the patch\n",
    "    \"\"\"\n",
    "    s = len(shape) - 2\n",
    "    num_activations = np.prod(shape[:s]) * np.sum(shape[s:])  # input_activations + output_activations\n",
    "    if dist == 'uniform':\n",
    "        lim = np.sqrt(6. / num_activations)\n",
    "        if lambda_initializer:\n",
    "            return np.random.uniform(-lim, lim, shape).astype(np.float32)\n",
    "        else:\n",
    "            return tf.random_uniform(shape, minval=-lim, maxval=lim)\n",
    "    if dist == 'normal':\n",
    "        stddev = np.sqrt(3. / num_activations)\n",
    "        if lambda_initializer:\n",
    "            return np.random.normal(0, stddev, shape).astype(np.float32)\n",
    "        else:\n",
    "            tf.truncated_normal(shape, mean=0, stddev=stddev)\n",
    "    raise ValueError('Distribution must be either \"uniform\" or \"normal\".')\n",
    "\n",
    "\n",
    "def constant_initializer(value, shape, lambda_initializer=True):\n",
    "    if lambda_initializer:\n",
    "        return np.full(shape, value).astype(np.float32)\n",
    "    else:\n",
    "        return tf.constant(value, tf.float32, shape)\n",
    "\n",
    "\n",
    "def get_spatial_rank(x):\n",
    "    \"\"\"\n",
    "    :param x: an input tensor with shape [batch_size, ..., num_channels]\n",
    "    :return: the spatial rank of the tensor i.e. the number of spatial dimensions between batch_size and num_channels\n",
    "    \"\"\"\n",
    "    return len(x.get_shape()) - 2\n",
    "\n",
    "\n",
    "def get_num_channels(x):\n",
    "    \"\"\"\n",
    "    :param x: an input tensor with shape [batch_size, ..., num_channels]\n",
    "    :return: the number of channels of x\n",
    "    \"\"\"\n",
    "    return int(x.get_shape()[-1])\n",
    "\n",
    "\n",
    "def get_spatial_size(x):\n",
    "    \"\"\"\n",
    "    :param x: an input tensor with shape [batch_size, ..., num_channels]\n",
    "    :return: The spatial shape of x, excluding batch_size and num_channels.\n",
    "    \"\"\"\n",
    "    return x.get_shape()[1:-1]\n",
    "\n",
    "\n",
    "# parametric leaky relu\n",
    "def prelu(x):\n",
    "    alpha = tf.get_variable('alpha', shape=x.get_shape()[-1], dtype=x.dtype, initializer=tf.constant_initializer(0.1))\n",
    "    return tf.maximum(0.0, x) + alpha * tf.minimum(0.0, x)\n",
    "\n",
    "\n",
    "def convolution(x, filter, padding='SAME', strides=None, dilation_rate=None):\n",
    "    w = tf.get_variable(name='weights', initializer=xavier_initializer_convolution(shape=filter))\n",
    "    b = tf.get_variable(name='biases', initializer=constant_initializer(0, shape=filter[-1]))\n",
    "\n",
    "    return tf.nn.convolution(x, w, padding, strides, dilation_rate) + b\n",
    "\n",
    "\n",
    "def deconvolution(x, filter, output_shape, strides, padding='SAME'):\n",
    "    w = tf.get_variable(name='weights', initializer=xavier_initializer_convolution(shape=filter))\n",
    "    b = tf.get_variable(name='biases', initializer=constant_initializer(0, shape=filter[-2]))\n",
    "\n",
    "    spatial_rank = get_spatial_rank(x)\n",
    "    if spatial_rank == 2:\n",
    "        return tf.nn.conv2d_transpose(x, filter, output_shape, strides, padding) + b\n",
    "    if spatial_rank == 3:\n",
    "        return tf.nn.conv3d_transpose(x, w, output_shape, strides, padding) + b\n",
    "    raise ValueError('Only 2D and 3D images supported.')\n",
    "\n",
    "\n",
    "# More complex blocks\n",
    "\n",
    "# down convolution\n",
    "def down_convolution(x, factor, kernel_size):\n",
    "    num_channels = get_num_channels(x)\n",
    "    spatial_rank = get_spatial_rank(x)\n",
    "    strides = spatial_rank * [factor]\n",
    "    filter = kernel_size + [num_channels, num_channels * factor]\n",
    "    x = convolution(x, filter, strides=strides)\n",
    "    return x\n",
    "\n",
    "\n",
    "# up convolution\n",
    "def up_convolution(x, output_shape, factor, kernel_size):\n",
    "    num_channels = get_num_channels(x)\n",
    "    spatial_rank = get_spatial_rank(x)\n",
    "    strides = [1] + spatial_rank * [factor] + [1]\n",
    "    filter = kernel_size + [num_channels // factor, num_channels]\n",
    "    x = deconvolution(x, filter, output_shape, strides=strides)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Layers import convolution, down_convolution, up_convolution, get_num_channels\n",
    "\n",
    "\n",
    "def convolution_block(layer_input, num_convolutions, keep_prob, activation_fn):\n",
    "    x = layer_input\n",
    "    n_channels = get_num_channels(x)\n",
    "    for i in range(num_convolutions):\n",
    "        with tf.variable_scope('conv_' + str(i+1)):\n",
    "            x = convolution(x, [5, 5, 5, n_channels, n_channels])\n",
    "            if i == num_convolutions - 1:\n",
    "                x = x + layer_input\n",
    "            x = activation_fn(x)\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "    return x\n",
    "\n",
    "\n",
    "def convolution_block_2(layer_input, fine_grained_features, num_convolutions, keep_prob, activation_fn):\n",
    "\n",
    "    x = tf.concat((layer_input, fine_grained_features), axis=-1)\n",
    "    n_channels = get_num_channels(layer_input)\n",
    "    if num_convolutions == 1:\n",
    "        with tf.variable_scope('conv_' + str(1)):\n",
    "            x = convolution(x, [5, 5, 5, n_channels * 2, n_channels])\n",
    "            x = x + layer_input\n",
    "            x = activation_fn(x)\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "        return x\n",
    "\n",
    "    with tf.variable_scope('conv_' + str(1)):\n",
    "        x = convolution(x, [5, 5, 5, n_channels * 2, n_channels])\n",
    "        x = activation_fn(x)\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "    for i in range(1, num_convolutions):\n",
    "        with tf.variable_scope('conv_' + str(i+1)):\n",
    "            x = convolution(x, [5, 5, 5, n_channels, n_channels])\n",
    "            if i == num_convolutions - 1:\n",
    "                x = x + layer_input\n",
    "            x = activation_fn(x)\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class VNet(object):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 keep_prob=1.0,\n",
    "                 num_channels=16,\n",
    "                 num_levels=4,\n",
    "                 num_convolutions=(1, 2, 3, 3),\n",
    "                 bottom_convolutions=3,\n",
    "                 activation_fn=tf.nn.relu):\n",
    "        \"\"\"\n",
    "        Implements VNet architecture https://arxiv.org/abs/1606.04797\n",
    "        :param num_classes: Number of output classes.\n",
    "        :param keep_prob: Dropout keep probability, set to 1.0 if not training or if no dropout is desired.\n",
    "        :param num_channels: The number of output channels in the first level, this will be doubled every level.\n",
    "        :param num_levels: The number of levels in the network. Default is 4 as in the paper.\n",
    "        :param num_convolutions: An array with the number of convolutions at each level.\n",
    "        :param bottom_convolutions: The number of convolutions at the bottom level of the network.\n",
    "        :param activation_fn: The activation function.\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.keep_prob = keep_prob\n",
    "        self.num_channels = num_channels\n",
    "        assert num_levels == len(num_convolutions)\n",
    "        self.num_levels = num_levels\n",
    "        self.num_convolutions = num_convolutions\n",
    "        self.bottom_convolutions = bottom_convolutions\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def network_fn(self, x, is_training):\n",
    "\n",
    "        keep_prob = self.keep_prob if is_training else 1.0\n",
    "        # if the input has more than 1 channel it has to be expanded because broadcasting only works for 1 input\n",
    "        # channel\n",
    "        input_channels = int(x.get_shape()[-1])\n",
    "        with tf.variable_scope('vnet/input_layer'):\n",
    "            if input_channels == 1:\n",
    "                x = tf.tile(x, [1, 1, 1, 1, self.num_channels])\n",
    "            else:\n",
    "                x = self.activation_fn(convolution(x, [5, 5, 5, input_channels, self.num_channels]))\n",
    "\n",
    "        features = list()\n",
    "\n",
    "        for l in range(self.num_levels):\n",
    "            with tf.variable_scope('vnet/encoder/level_' + str(l + 1)):\n",
    "                x = convolution_block(x, self.num_convolutions[l], keep_prob, activation_fn=self.activation_fn)\n",
    "                features.append(x)\n",
    "                with tf.variable_scope('down_convolution'):\n",
    "                    x = self.activation_fn(down_convolution(x, factor=2, kernel_size=[2, 2, 2]))\n",
    "\n",
    "        with tf.variable_scope('vnet/bottom_level'):\n",
    "            x = convolution_block(x, self.bottom_convolutions, keep_prob, activation_fn=self.activation_fn)\n",
    "\n",
    "        for l in reversed(range(self.num_levels)):\n",
    "            with tf.variable_scope('vnet/decoder/level_' + str(l + 1)):\n",
    "                f = features[l]\n",
    "                with tf.variable_scope('up_convolution'):\n",
    "                    x = self.activation_fn(up_convolution(x, tf.shape(f), factor=2, kernel_size=[2, 2, 2]))\n",
    "\n",
    "                x = convolution_block_2(x, f, self.num_convolutions[l], keep_prob, activation_fn=self.activation_fn)\n",
    "\n",
    "        with tf.variable_scope('vnet/output_layer'):\n",
    "            logits = convolution(x, [1, 1, 1, self.num_channels, self.num_classes])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Diogo Amorim, 2018-07-10\n",
    "V-Net implementation in Keras 2\n",
    "https://arxiv.org/pdf/1606.04797.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Deconvolution3D(Layer):\n",
    "    def __init__(self, filters, kernel_size, output_shape, subsample):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = (1,) + subsample + (1,)\n",
    "        self.output_shape_ = output_shape\n",
    "        assert K.backend() == 'tensorflow'\n",
    "        super(Deconvolution3D, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 5\n",
    "        self.input_shape_ = input_shape\n",
    "        W_shape = self.kernel_size + (self.filters, input_shape[4],)\n",
    "        self.W = self.add_weight(W_shape,\n",
    "                                 initializer=functools.partial(initializers.glorot_uniform()),\n",
    "                                 name='{}_W'.format(self.name))\n",
    "        self.b = self.add_weight((1, 1, 1, self.filters,), initializer='zero', name='{}_b'.format(self.name))\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None,) + self.output_shape_[1:]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return tf.nn.conv3d_transpose(x, self.W, output_shape=self.output_shape_,\n",
    "                                      strides=self.strides, padding='SAME', name=self.name) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(Deconvolution3D, self).get_config().copy()\n",
    "        base_config['output_shape'] = self.output_shape_\n",
    "        return base_config\n",
    "\n",
    "\n",
    "def downward_layer(input_layer, n_convolutions, n_output_channels):\n",
    "    inl = input_layer\n",
    "\n",
    "    for _ in range(n_convolutions):\n",
    "        inl = PReLU()(\n",
    "            Conv3D(filters=(n_output_channels // 2), kernel_size=5,\n",
    "                   padding='same', kernel_initializer='he_normal')(inl)\n",
    "        )\n",
    "    add_l = add([inl, input_layer])\n",
    "    downsample = Conv3D(filters=n_output_channels, kernel_size=2, strides=2,\n",
    "                        padding='same', kernel_initializer='he_normal')(add_l)\n",
    "    downsample = PReLU()(downsample)\n",
    "    return downsample, add_l\n",
    "\n",
    "\n",
    "def upward_layer(input0, input1, n_convolutions, n_output_channels):\n",
    "    merged = concatenate([input0, input1], axis=4)\n",
    "    inl = merged\n",
    "    for _ in range(n_convolutions):\n",
    "        inl = PReLU()(\n",
    "            Conv3D((n_output_channels * 4), kernel_size=5,\n",
    "                   padding='same', kernel_initializer='he_normal')(inl)\n",
    "        )\n",
    "    add_l = add([inl, merged])\n",
    "    shape = add_l.get_shape().as_list()\n",
    "    new_shape = (1, shape[1] * 2, shape[2] * 2, shape[3] * 2, n_output_channels)\n",
    "    upsample = Deconvolution3D(n_output_channels, (2, 2, 2), new_shape, subsample=(2, 2, 2))(add_l)\n",
    "    return PReLU()(upsample)\n",
    "\n",
    "\n",
    "def vnet(input_size=(128, 128, 128, 1), optimizer=Adam(lr=1e-4),\n",
    "         loss='binary_crossentropy', metrics=['accuracy']):\n",
    "         # loss='categorical_crossentropy', metrics=['categorical_accuracy']):\n",
    "    # Layer 1\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv3D(16, kernel_size=5, strides=1, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = PReLU()(conv1)\n",
    "    repeat1 = concatenate(16 * [inputs], axis=-1)\n",
    "    add1 = add([conv1, repeat1])\n",
    "    down1 = Conv3D(32, 2, strides=2, padding='same', kernel_initializer='he_normal')(add1)\n",
    "    down1 = PReLU()(down1)\n",
    "\n",
    "    # Layer 2,3,4\n",
    "    down2, add2 = downward_layer(down1, 2, 64)\n",
    "    down3, add3 = downward_layer(down2, 3, 128)\n",
    "    down4, add4 = downward_layer(down3, 3, 256)\n",
    "\n",
    "    # Layer 5\n",
    "    # !Mudar kernel_size=(5, 5, 5) quando imagem > 64!\n",
    "    conv_5_1 = Conv3D(256, kernel_size=(5, 5, 5), padding='same', kernel_initializer='he_normal')(down4)\n",
    "    conv_5_1 = PReLU()(conv_5_1)\n",
    "    conv_5_2 = Conv3D(256, kernel_size=(5, 5, 5), padding='same', kernel_initializer='he_normal')(conv_5_1)\n",
    "    conv_5_2 = PReLU()(conv_5_2)\n",
    "    conv_5_3 = Conv3D(256, kernel_size=(5, 5, 5), padding='same', kernel_initializer='he_normal')(conv_5_2)\n",
    "    conv_5_3 = PReLU()(conv_5_3)\n",
    "    add5 = add([conv_5_3, down4])\n",
    "    aux_shape = add5.get_shape()\n",
    "    upsample_5 = Deconvolution3D(128, (2, 2, 2), (1, aux_shape[1].value*2,aux_shape[2].value*2, aux_shape[3].value*2, 128), subsample=(2, 2, 2))(add5)\n",
    "    upsample_5 = PReLU()(upsample_5)\n",
    "\n",
    "    # Layer 6,7,8\n",
    "    upsample_6 = upward_layer(upsample_5, add4, 3, 64)\n",
    "    upsample_7 = upward_layer(upsample_6, add3, 3, 32)\n",
    "    upsample_8 = upward_layer(upsample_7, add2, 2, 16)\n",
    "\n",
    "    # Layer 9\n",
    "    merged_9 = concatenate([upsample_8, add1], axis=4)\n",
    "    conv_9_1 = Conv3D(32, kernel_size=(5, 5, 5), padding='same', kernel_initializer='he_normal')(merged_9)\n",
    "    conv_9_1 = PReLU()(conv_9_1)\n",
    "    add_9 = add([conv_9_1, merged_9])\n",
    "    # conv_9_2 = Conv3D(1, kernel_size=(1, 1, 1), padding='same', kernel_initializer='he_normal')(add_9)\n",
    "    conv_9_2 = Conv3D(1, kernel_size=(1, 1, 1), padding='same', kernel_initializer='he_normal')(add_9)\n",
    "    conv_9_2 = PReLU()(conv_9_2)\n",
    "\n",
    "    # softmax = Softmax()(conv_9_2)\n",
    "    sigmoid = Conv3D(1, kernel_size=(1, 1, 1), padding='same', kernel_initializer='he_normal',\n",
    "                     activation='sigmoid')(conv_9_2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=sigmoid)\n",
    "    # model = Model(inputs=inputs, outputs=softmax)\n",
    "\n",
    "    model.compile(optimizer, loss, metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = vnet()\n",
    "# model.summary(line_length=133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VNet(SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_input = tf.placeholder(dtype=tf.float32, shape=(SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 4 but is rank 5 for 'vnet/input_layer_1/Tile' (op: 'Tile') with input shapes: [193,229,193,1], [5].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1864\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 4 but is rank 5 for 'vnet/input_layer_1/Tile' (op: 'Tile') with input shapes: [193,229,193,1], [5].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a7759ef4e85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-94f118da0791>\u001b[0m in \u001b[0;36mnetwork_fn\u001b[0;34m(self, x, is_training)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnet/input_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_channels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  10901\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10902\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m> 10903\u001b[0;31m         \"Tile\", input=input, multiples=multiples, name=name)\n\u001b[0m\u001b[1;32m  10904\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10905\u001b[0m     result = _dispatch.dispatch(\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3616\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2025\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   2026\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 2027\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 4 but is rank 5 for 'vnet/input_layer_1/Tile' (op: 'Tile') with input shapes: [193,229,193,1], [5]."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "La segmentation automatique.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
